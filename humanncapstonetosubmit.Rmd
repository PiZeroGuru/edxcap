---
title: "Humann_Edx_Capstone_CYO"
author: "James Humann"
date: "April 30, 2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```



```{r download, include = FALSE, echo=FALSE}

#############################################################
# Create edx set, validation set, and submission file
#############################################################
#
#PLEASE PUT .R .zip .rmd ALL IN SAME WORKING DIRECTORY TO RUN CODE
#


#Initializing
library(ggplot2)
library(tidyverse)
library(readr)
library(caret)

#Loading data
rm(list=ls()) 
df <- read_csv("nyc-property-sales.zip")
```
# Introduction
This report analyses the New York City Property Sales dataset available from (https://www.kaggle.com/new-york-city/nyc-property-sales), which contains over 200,000 records of property sales within New York City in the year 2017. The data set contains records of the sale price, square footage, borough, neighborhood, lot, tax class, and building class.

We analyze this data set in an effort to predict selling prices. This can be a very beneficial prediction, as property is inherently unique, and thus difficult to value objectively. Because of the immense prices (sales were routinely in the millions of dollars), this can be the most expensive purchase that a family makes in their lifetime, and thus overpaying even by a few percent can be disastrous.

To do our analysis, we perform data cleaning, then exploratory analysis to identify trends. Finally we split our data into a training set (90% of data) and test set (10% of data), so that we can train a linear model on the training set and evaluate its performance in the test set.

We focus our data analysis on predicting Price per Square Foot (PPSQFT), so that the results can be independent of property size, which is obviously a large determining factor in the final price. The final price can always be determined from the PPSQFT since the square footage of the property will be known before sale.

# Data Cleaning
The original data contains many nonsensical sales, such as $0 or $100 sales. It also includes many rows with missing data for the square footage or price. We remove all of these since our anlysis will be based on price per square foot. We also decide to filter out sales below $100,000 since it is not possible to buy a family home or business for these prices in New York City. The orginal and cleaned data sets are shown here.

```{r}
#original
head(df)
```

```{r cleaning, include=FALSE, echo = FALSE}
#Filter out nonsensical and cheap sales <$100000, filter out rows with missing price or square footage, filter out square footage below 1000
#Add price per square foot column, since that's what we're truly interested in. Otherwise the data is strongly affected by building size
#The full price can always be calculated afterwards since the square footage will be known
buildingSales <- df %>% select(BOROUGH,NEIGHBORHOOD,`BUILDING CLASS CATEGORY`,`LAND SQUARE FEET`,`GROSS SQUARE FEET`,`SALE PRICE`) %>%
            filter(grepl("[0123456789]{6}",.$'SALE PRICE'), grepl("[0123456789]{3}",.$'LAND SQUARE FEET'), grepl("[0123456789]{3}",.$'GROSS SQUARE FEET')) %>%
            mutate('SALE PRICE' = as.numeric(.$'SALE PRICE'),BOROUGH = as.factor(.$BOROUGH), NEIGHBORHOOD = as.factor(NEIGHBORHOOD), 'BUILDING CLASS CATEGORY' = as.factor(.$'BUILDING CLASS CATEGORY'), 'LAND SQUARE FEET' = as.numeric(.$`LAND SQUARE FEET`), 'GROSS SQUARE FEET' = as.numeric(.$'GROSS SQUARE FEET')) %>%
            mutate(PPSQFT = .$'SALE PRICE'/.$'GROSS SQUARE FEET')
head(buildingSales)

#Rename without spaces
names(buildingSales) <- c("BOROUGH", "NEIGHBORHOOD", "CLASS", "LAND_SQFT", "GROSS_SQFT", "PRICE", "PPSQFT")

#Divide land sizes into XSmall (smallest 5%) and XLarge (largest 5%). Divide middle 90% evenly into small, medium, and large
lotSizeQuintiles <- quantile(buildingSales$LAND_SQFT, probs =  c(0.0, 0.05, 0.35, 0.65, 0.95, 1.00))#c(0.0, 0.20, 0.40, 0.60, 0.80, 1.00))
sizeLots <- function(toBe, levs){
                        if(toBe <= levs[2]){'XSmall'}
                        else{if (toBe <= levs[3]){'Small'}
                          else{if (toBe <= levs[4]){'Medium'}
                            else{if (toBe <= levs[5]){'Large'}
                              else{'XLarge'}}}}
}
buildingSales <- buildingSales %>%  mutate(LOT_SIZE = sapply(LAND_SQFT, FUN = sizeLots, levs = lotSizeQuintiles)) %>% mutate(LOT_SIZE = as.factor(LOT_SIZE)) #rowwise() %>% mutate(LOT_SIZE = sizeLots(LAND_SQFT,lotSizeQuintiles))
head(buildingSales)
```
```{r}
#cleaned
head(buildingSales)
```

# Analysis
Here we perform exploratory analysis and then settle on a linear model to predict PPSQFT.  

**Exploratory Analysis** 


```{R exploratory, include = FALSE, echo = FALSE}
#Exploratory analysis
unique(buildingSales$BOROUGH)
unique(buildingSales$NEIGHBORHOOD)
unique(buildingSales$CLASS)

length(unique(buildingSales$BOROUGH))
length(unique(buildingSales$NEIGHBORHOOD))
length(unique(buildingSales$CLASS))

#Exploratory plots
#Many plots are hard to view due to outliers (e.g. billion+ dollar sales and million+ sqft lot sizes make regular sales impossible to see)
#So some are followed by xlim and ylim to zoom in
buildingSales %>% ggplot(aes(BOROUGH)) + geom_bar()
buildingSales %>% ggplot(aes(NEIGHBORHOOD)) + geom_bar()  + theme(axis.text.x=element_text(angle=90, hjust=1))
buildingSales %>% ggplot(aes(CLASS)) + geom_bar()  + theme(axis.text.x=element_text(angle=90, hjust=1))
buildingSales %>% ggplot(aes(PRICE)) + geom_histogram()
buildingSales %>% ggplot(aes(x = GROSS_SQFT, y = PRICE)) + geom_point()
single_fam_sqft <- buildingSales %>% filter(CLASS == '01 ONE FAMILY DWELLINGS') %>% ggplot(aes(x = GROSS_SQFT, y = PRICE)) + geom_point() + xlim(0,10000) + ylim(0,10000000)
price_hist_lotsize <- buildingSales %>% filter(LAND_SQFT < 25000) %>% ggplot(aes(LAND_SQFT)) + geom_histogram()
price_hist_mil <- buildingSales %>% filter(PRICE <= 1000000) %>% ggplot(aes(PRICE)) + geom_histogram()

buildingSales %>% group_by(BOROUGH) %>% ggplot(aes(x = BOROUGH, y = PRICE)) + geom_boxplot()
bp_price <- buildingSales %>% group_by(BOROUGH) %>% ggplot(aes(x = BOROUGH, y = PRICE)) + geom_boxplot() + ylim(0,10000000)
bp_ppsqft <- buildingSales %>% group_by(BOROUGH) %>% ggplot(aes(x = BOROUGH, y = PPSQFT)) + geom_boxplot()  + ylim(0,10000)

buildingSales %>% filter(BOROUGH == 1) %>% select(PRICE) %>% summary()
topTenNeighborhoods <- buildingSales %>% group_by(NEIGHBORHOOD) %>% summarize(countNeighborhood = n()) %>% arrange(desc(countNeighborhood)) %>% top_n(10) %>% pull(NEIGHBORHOOD)
topTenNeighborhoods
neigh_plot <- buildingSales %>% filter(NEIGHBORHOOD %in% topTenNeighborhoods) %>% group_by(NEIGHBORHOOD) %>% ggplot(aes(x = NEIGHBORHOOD, y = PPSQFT)) + geom_boxplot() + theme(axis.text.x=element_text(angle=90, hjust=1)) + ylim(0,5000)
```
The distribution of sales prices is quite wide, making histograms hard to read. Here we calculate the maximum sale price and show a histogram of prices below $1,000,000:
```{r}
max(buildingSales$PRICE)
price_hist_mil
```

We test our assumption that we can focus on PPSQFT instead of the overall price by plotting them against each other for single-family homes below $10,000,000. We see a somewhat linear trend that is nonetheless very noisy so will need advanced machine learning techniques to be stratified by borough, neighborhood, and/or building class.
```{r}
single_fam_sqft
```
The following box plots show that borough and neighborhood (only top 10 by number of sales are shown) are good stratifiers for predicing price and price per square foot:
```{r, echo=FALSE}
bp_price
bp_ppsqft
neigh_plot
```

**Linear Model**


From our exploratory analysis, it appears that a linear model based on the boroughs and neighborhoods can give us predictive power, so we train a linear model to predict PPSQFT based on these two variables. We also attempted to refine the predictions using building class or lot size, but surprisingly these actually hindered the performance of our system.
```{r linear, include = FALSE, echo = FALSE}
#Set seed for repeatability, runs well with other seeds too
set.seed(1)

#Creating data partition and ensuring that all neighborhoods and building classes in test validation set are present in test set
test_index <- createDataPartition(y = buildingSales$PPSQFT, times = 1, p = 0.1, list = FALSE)

trainSet <- buildingSales[-test_index,]
testSet <- buildingSales[test_index,]

head(trainSet)
head(testSet)


unique(trainSet$BOROUGH)
unique(trainSet$NEIGHBORHOOD)
unique(trainSet$CLASS)

length(unique(trainSet$BOROUGH))
length(unique(trainSet$NEIGHBORHOOD))
length(unique(trainSet$CLASS))

unique(testSet$BOROUGH)
unique(testSet$NEIGHBORHOOD)
unique(testSet$CLASS)

length(unique(testSet$BOROUGH))
length(unique(testSet$NEIGHBORHOOD))
length(unique(testSet$CLASS))

validation <- testSet %>%
  semi_join(trainSet, by = "NEIGHBORHOOD") %>%
  semi_join(trainSet, by = "CLASS")

removed <- anti_join(testSet, validation)
trainSet <- rbind(trainSet, removed)

mean(validation$BOROUGH %in% trainSet$BOROUGH)
mean(validation$NEIGHBORHOOD %in% trainSet$NEIGHBORHOOD)
mean(validation$CLASS %in% trainSet$CLASS)
# ^^^ needs to be 1.00

mean(trainSet$BOROUGH %in% validation$BOROUGH)
mean(trainSet$NEIGHBORHOOD %in% validation$NEIGHBORHOOD)
mean(trainSet$CLASS %in% validation$CLASS)

unique(validation$BOROUGH)
unique(validation$NEIGHBORHOOD)
unique(validation$CLASS)

length(unique(validation$BOROUGH))
length(unique(validation$NEIGHBORHOOD))
length(unique(validation$CLASS))

#Exploratory analysis shows that boroughs, neighborhoods, land size, and building class could all be important
#Creating linear models predicting based off of above factors

linearFitB <- lm(PPSQFT ~ BOROUGH, data = buildingSales)
yhatB <- predict(linearFitB, validation)
RMSEB <- RMSE(yhatB,validation$PPSQFT)
results <- data.frame(fit = "BOROUGH", err = RMSEB, stringsAsFactors = FALSE)

linearFitBN <- lm(PPSQFT ~ BOROUGH + NEIGHBORHOOD, data = buildingSales)
yhatBN <- predict(linearFitBN, validation)
RMSEBN <- RMSE(yhatBN,validation$PPSQFT)
results <- rbind(results, c("BOROUGH + NEIGHBORHOOD", RMSEBN))

#Seems that adding factors beyond neighborhood doesn't help
linearFitBNL <- lm(PPSQFT ~ BOROUGH + NEIGHBORHOOD + LOT_SIZE, data = buildingSales)
yhatBNL <- predict(linearFitBNL, validation)
RMSEBNL <- RMSE(yhatBNL,validation$PPSQFT)
results <- rbind(results, c("BOROUGH + NEIGHBORHOOD + LOT_SIZE", RMSEBNL))

linearFitBNC <- lm(PPSQFT ~ BOROUGH + NEIGHBORHOOD + CLASS, data = buildingSales)
yhatBNC <- predict(linearFitBNC, validation)
RMSEBNC <- RMSE(yhatBNC,validation$PPSQFT)
results <- rbind(results, c("BOROUGH + NEIGHBORHOOD + CLASS", RMSEBNC))
```
#Results
The following RMSE values show the performance of the four linear models we tested:
```{r}
results
```
We choose the linear model based on neighborhood and borough, as it gives the lowest RMSE of `r results$err[2]`. This RMSE corresponds to an error of about `r sqrt(as.numeric(results$err[2]))` in the price per square foot.

#Conclusion
In our analysis, simpler is better, as the most accurate linear model to predict PPSQFT used on the borough and neighborhood of the sale. Future work may be able to refine these predictions not with a linear model, but with a nearest-neighbors algorithm, which would find the average sales price of similar properties. This is akin to what home-buyers and agents currently do manually, comparing "comps," or the sales price of comparable properties.

Buying and selling property, especially in global cities such as New York, will continue to be a major investment that could make or break a family or business's finances. Using advanced machine learning tools to ensure that one is getting a fair price should become standard practice in the modern market.